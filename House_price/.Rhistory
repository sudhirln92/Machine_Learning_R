#Mercedes benz
library(randomForest)
library(dplyr)
library(magrittr)
#Data import
setwd('/home/sudhir/R/Benz')
train<-read.csv('train.csv',na.strings = 'NA',stringsAsFactors = F)
test<-read.csv('test.csv',na.strings = 'NA',stringsAsFactors = F)
test$y<-NULL
full<-bind_rows(train,test)
#Data manipulation
table(is.na(full))
library(xgboost)
param<-list('objective'='binary:logistic',
'eval_metric'='logloss',
'eta'=1, 'max.depth'=2)
bst.cv<-xgb.cv(params = param,data=as.matrix(train),nflod=10,nround=20)
bst.cv<-xgb.cv(params = param,data=as.matrix(train),lable=train$y nflod=10,nround=20)
bst.cv<-xgb.cv(params = param,data=as.matrix(train),lable=train$y nfold=10,nround=20)
bst.cv<-xgb.cv(params = param,data=as.matrix(train),lable=train$y, nfold=10,nround=20)
bst.cv<-xgb.cv(params = param,data=as.matrix(train),lable=as.matrix(train$y), nfold=10,nround=20)
xg.m<-xgboost(data = as.matrix(train), label = train$y, max.depth = 2, eta = 1, nround = 5,
nthread = 2, objective = "binary:logistic")
x_train<-xgb.DMatrix(data=as.matrix(train),lable=train[,'y'])
x_test<-xgb.DMatrix(data=as.matrix(test))
#Mercedes benz
library(randomForest)
library(dplyr)
library(magrittr)
#Data import
setwd('/home/sudhir/R/Benz')
train<-read.csv('train.csv',na.strings = 'NA',stringsAsFactors = F)
test<-read.csv('test.csv',na.strings = 'NA',stringsAsFactors = F)
test$y<-NULL
full<-bind_rows(train,test)
#Data manipulation
table(is.na(full))
colSums(is.na(full)) #y variable in test data contains missing values
str(full)
summary(full)
#nzv<-nearZeroVar(full)
num_var=names(full)[sapply(full,is.integer)]
cat_var=names(full)[sapply(full,is.factor)]
#Convert numeric variable to factor
#full[cat_var]<-lapply(full[cat_var], integer)
full$X1=as.factor(full$X1)
full$X3=as.factor(full$X3)
full$X4=as.factor(full$X4)
full$X8=as.factor(full$X8)
#full %<>%
#  mutate_each_(funs(integer(.)),cat_var)
#full$ID=as.integer(full$ID)
str(full)
#Data sampling
train1=full[1:4209,]
test1=full[4210:8418,]
library(rpart)
dc.model<-lm(y~., data=train1)
dc.model$xlevels<-union(dc.model$xlevels,levels(test1))
plot(dc.model)
prd<-predict(dc.model,test1)
prd<-predict(dc.model,test1,type='class')
prd<-predict(dc.model,test1,type='repo')
prd<-predict(dc.model,test1,type='response')
x_train<-xgb.DMatrix(data=as.matrix(train),lable=train[,'y'])
for(i in 1:378){
if(is.factor(full1[,i])){
full1[,i]<-as.integer(full1[,i])
}
}
full[,i]<-as.integer(full[,i])
str(full)
if(is.factor(full[,i])|is.character(full[,i])){
full[,i]<-as.integer(full[,i])
}
str(full)
train1=full[1:4209,]
test1=full[4210:8418,]
x_train<-xgb.DMatrix(data=as.matrix(train),lable=train[,'y'])
x_test<-xgb.DMatrix(data=as.matrix(test))
x_train<-xgb.DMatrix(data=as.matrix(train1),lable=train1[,'y'])
x_test<-xgb.DMatrix(data=as.matrix(test1))
param<-list('objective'='binary:logistic',
'eval_metric'='logloss',
'eta'=1, 'max.depth'=2)
bst.cv<-xgb.cv(params = param,data=as.matrix(train1),lable=as.matrix(train1$y), nfold=10,nround=20)
xg.m<-xgboost(data = as.matrix(train), label = train$y, max.depth = 2, eta = 1, nround = 5,
nthread = 2, objective = "binary:logistic")
#House prices
library(ggplot2)
library(randomForest)
library(caret)
#Data import
setwd("/home/sudhir/git/ML-R/House_price")
train<-read.csv('train.csv',na.strings = c("","-"," ","NA","na","Na","_"))
test<-read.csv('test.csv',na.strings = c("","-"," ","NA","na","Na","_"))
test$SalePrice<-'none'
table(test$SalePrice)
full<-rbind(train,test)
str(full)
numeric_var<-names(full)[which(sapply(full,is.numeric))]
cat_var<-names(full)[which(sapply(full,is.factor))]
nzv<-nearZeroVar(full[numeric_var],)
#finding missing values
mis<-table(is.na(full))
mis
mis[2]*100/(nrow(full)*ncol(full))
colSums(is.na(full))
colSums(is.na(full[,numeric_var]))
#Replacing missing value with mean
table(is.na(full$LotFrontage))
full[sapply(full,is.numeric)]<-lapply(full[sapply(full,is.numeric)],function(x) ifelse(is.na(x),mean(x,na.rm=T),x))
full$LotFrontage[is.na(full$LotFrontage)]=mean(full$LotFrontage,na.rm =TRUE)
table(is.na(full$MasVnrArea))
full$MasVnrArea[is.na(full$MasVnrArea)]=mean(full$MasVnrArea,na.rm =TRUE)
table(is.na(full$GarageYrBlt))
full$GarageYrBlt[is.na(full$GarageYrBlt)]=mean(full$GarageYrBlt,na.rm=TRUE)
table(is.na(full$BsmtFinSF1))
full$BsmtFinSF1[is.na(full$BsmtFinSF1)]=mean(full$BsmtFinSF1,na.rm=TRUE)
table(is.na(full$BsmtFinSF2))
full$BsmtFinSF2[is.na(full$BsmtFinSF2)]=mean(full$BsmtFinSF2,na.rm=TRUE)
table(is.na(full$TotalBsmtSF))
full$TotalBsmtSF[is.na(full$TotalBsmtSF)]=mean(full$TotalBsmtSF,na.rm=TRUE)
table(is.na(full$BsmtFullBath))
full$BsmtFullBath[is.na(full$BsmtFullBath)]=mean(full$BsmtFullBath,na.rm=TRUE)
table(is.na(full$BsmtHalfBath))
full$BsmtHalfBath[is.na(full$BsmtHalfBath)]=mean(full$BsmtHalfBath,na.rm=TRUE)
table(is.na(full$BsmtUnfSF))
full$BsmtUnfSF[is.na(full$BsmtUnfSF)]=mean(full$BsmtUnfSF,na.rm=TRUE)
table(is.na(full$GarageCars))
full$GarageCars[is.na(full$GarageCars)]=mean(full$GarageCars,na.rm=TRUE)
table(is.na(full$GarageArea))
full$GarageArea[is.na(full$GarageArea)]=mean(full$GarageArea,na.rm=TRUE)
#Finding missing value in catagorical variable
colSums(is.na(full[cat_var]))
summary(full$MSZoning)
full$MSZoning[is.na(full$MSZoning)]='RL'
summary(full$MasVnrType)
full$MasVnrType[is.na(full$MasVnrType)]='None'
summary(full$Utilities)
full$Utilities[is.na(full$Utilities)]='AllPub'
summary(full$Exterior1st)
full$Exterior1st[is.na(full$Exterior1st)]='VinylSd'
summary(full$Exterior2nd)
full$Exterior2nd[is.na(full$Exterior2nd)]='VinylSd'
summary(full$BsmtQual)
full$BsmtQual[is.na(full$BsmtQual)]='Gd'
summary(full$BsmtCond)
full$BsmtCond[is.na(full$BsmtCond)]='TA'
summary(full$BsmtExposure)
full$BsmtExposure[is.na(full$BsmtExposure)]='No'
summary(full$BsmtFinType1)
full$BsmtFinType1[is.na(full$BsmtFinType1)]='Unf'
summary(full$BsmtFinType2)
full$BsmtFinType2[is.na(full$BsmtFinType2)]='Unf'
summary(full$Electrical)
full$Electrical[is.na(full$Electrical)]='SBrkr'
summary(full$GarageType)
full$GarageType[is.na(full$GarageType)]='Attchd'
summary(full$GarageFinish)
full$GarageFinish[is.na(full$GarageFinish)]='Unf'
summary(full$GarageCond)
full$GarageCond[is.na(full$GarageCond)]='TA'
summary(full$GarageQual)
full$GarageQual[is.na(full$GarageQual)]='TA'
summary(full$KitchenQual)
full$KitchenQual[is.na(full$KitchenQual)]='TA'
summary(full$Functional)
full$Functional[is.na(full$Functional)]='Typ'
summary(full$SaleType)
full$SaleType[is.na(full$SaleType)]='WD'
summary(full$FireplaceQu)
table(as.factor(full$Fireplaces), useNA = "ifany")
#Fireplaces is
levels(full$FireplaceQu)<-c(levels(full$FireplaceQu),'none')
full$FireplaceQu[is.na(full$FireplaceQu)]<-rep('none')
#Missing value present in this varaible in more than 90%, so we can ignore this variable
summary(full$Alley)
summary(full$PoolQC)
summary(full$Fence)
summary(full$MiscFeature)
c('Alley','PoolQC','Fence','MiscFeature') #ignore
colSums(is.na(full))
full1=full[,-c(7,73,74,75)]
#full1[]<-lapply(full1,as.numeric)
for(i in 1:77){
if(is.factor(full1[,i])){
full1[,i]<-as.integer(full1[,i])
}
}
train1=full1[1:1460,]
test1=full1[1461:2919,1:76]
train1$SalePrice=as.integer(train1$SalePrice)
library(gbm)
model <- gbm(SalePrice ~., data = train1, distribution = "laplace",
shrinkage = 0.05,
interaction.depth = 5,
bag.fraction = 0.66,
n.minobsinnode = 1,
cv.folds = 100,
keep.data = F,
verbose = F,
n.trees = 1000)
model <- gbm(SalePrice ~., data = train1, distribution = "laplace",
shrinkage = 0.05,
interaction.depth = 5,
bag.fraction = 0.6,
n.minobsinnode = 1,
cv.folds = 1,
keep.data = F,
verbose = F,
n.trees = 100)
model <- gbm(SalePrice ~., data = train1, distribution = "laplace",
shrinkage = 0.05,
interaction.depth = 5,
#bag.fraction = 0.6,
#n.minobsinnode = 1,
#cv.folds = 1,
keep.data = F,
verbose = F,
n.trees = 100)
model <- gbm(SalePrice ~., data = train1, distribution = "gaussian",
shrinkage = 0.05,
interaction.depth = 5,
#bag.fraction = 0.6,
#n.minobsinnode = 1,
#cv.folds = 1,
keep.data = F,
verbose = F,
n.trees = 100)
predict <- predict(model, test1, n.trees = 100)
submit=data.frame(Id=test1$Id,SalePrice=predict)
write.csv(submit,'submithouseprice.csv',row.names = F)
acc<-mean(predict==train1$SalePrice)
acc<-mean(ave( predict==train1$SalePrice))
acc<-mean(ave( predict &&train1$SalePrice))
confusionMatrix(model,test1)
library(xgboost)
xgtrain<-xgb.DMatrix(as.matrix(train1[,2:76]),label=train1[,"SalePrice"])
xgtest<-xgb.DMatrix(as.matrix(test1))
xgbcv <- xgb.cv( params = params, data = xgtrain, nrounds = 100, nfold = 5, showsd = T,
stratified = T, print.every.n = 10, early.stop.round = 20, maximize = F)
param<-list('objective=binary:logistic',
'eval_metric=logloss','eta=1',
'max_depth'=2)
xgbcv <- xgb.cv( params = params, data = xgtrain, nrounds = 100, nfold = 5, showsd = T,
stratified = T, print.every.n = 10, early.stop.round = 20, maximize = F)
xgbcv <- xgb.cv( params = params, data = xgtrain, nrounds = 100, nfold = 5, showsd = T,
stratified = T, print_every_n = 10, early_stop_round = 20, maximize = F)
params<-list('objective=binary:logistic',
'eval_metric=logloss','eta=1',
'max_depth'=2)
xgbcv <- xgb.cv( params = params, data = xgtrain, nrounds = 100, nfold = 5, showsd = T,
stratified = T, print_every_n = 10, early_stop_round = 20, maximize = F)
params<-list('objective'='binary:logistic',
'eval_metric'='logloss','eta'=1,
'max_depth'=2)
xgbcv <- xgb.cv( params = params, data = xgtrain, nrounds = 100, nfold = 5, showsd = T,
stratified = T, print_every_n = 10, early_stop_round = 20, maximize = F)
params<-list('objective'='reg:linear',
'eval_metric'='logloss','eta'=1,
'max_depth'=2)
xgbcv <- xgb.cv( params = params, data = xgtrain, nrounds = 100, nfold = 5, showsd = T,
stratified = T, print_every_n = 10, early_stop_round = 20, maximize = F)
xgb<-xgboost(data=xgtrain,label = train1[,"SalePrice"],
eta = 1,
max_depth = 15,
nround=100,
subsample = 0.5,
colsample_bytree = 0.5,
seed = 1,
eval_metric = "auc",
objective = "reg:linear",
num_class = 12,
nthread = 3)
params <- list(booster = "gbtree", objective = "reg:linear", eta=0.3, gamma=0,
max_depth=6, min_child_weight=1, subsample=1, colsample_bytree=1)
xgbcv <- xgb.cv( params = params, data = xgtrain, nrounds = 100, nfold = 5, showsd = T,
stratified = T, print_every_n = 10, early_stop_round = 20, maximize = F)
xgbcv <- xgb.cv( params = params, data = xgtrain, nrounds = 100, nfold = 5, showsd = T,
stratified = T, print_every_n = 5, early_stop_round = 20, maximize = F)
xgbcv <- xgb.cv( params = params, data = xgtrain, nrounds = 1000, nfold = 5, showsd = T,
stratified = T, print_every_n = 10, early_stop_round = 20, maximize = F)
plot(log(xgbcv$evaluation_log))
xgbm<-xgboost(data=xgtrain,params = params)
xgbm<-xgboost(data=xgtrain,params = params,nrounds = 1000,verbose = F)
xgprd<-predict(xgbm,xgtest)
confusionMatrix(model,test1)
submit=data.frame(Id=test1$Id,SalePrice=xgprd)
write.csv(submit,'xgsubmithouseprice.csv',row.names = F)
xgbm$callbacks$cb.save.model
summary(xgbm)
xgb.model.dt.tree(model=xgbm)
plot(xgb.model.dt.tree(model=xgbm))
xgb.model.dt.tree(model=xgbm,dimnames(train))
xgb.model.dt.tree(model=xgbm,dimnames(train$SalePrice))
